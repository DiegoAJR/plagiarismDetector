{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\diego\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module https://tfhub.dev/google/universal-sentence-encoder/4 loaded\n"
     ]
    }
   ],
   "source": [
    "! pip install -q tensorflow-hub\n",
    "! pip install -q seaborn\n",
    "from absl import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "from preprocess import preprocessing\n",
    "from preparation import preparation\n",
    "\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "model = hub.load(module_url)\n",
    "print (\"module %s loaded\" % module_url)\n",
    "def embed(input):\n",
    "  return model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: Elephant\n",
      "Embedding size: 512\n",
      "Embedding: [0.008344464004039764, 0.00048085112939588726, 0.06595247983932495, ...]\n",
      "\n",
      "Message: I am a sentence for which I would like to get its embedding.\n",
      "Embedding size: 512\n",
      "Embedding: [0.050808604806661606, -0.016524311155080795, 0.015737788751721382, ...]\n",
      "\n",
      "Message: Universal Sentence Encoder embeddings also support short paragraphs. There is no hard limit on how long the paragraph is. Roughly, the longer the more 'diluted' the embedding will be.\n",
      "Embedding size: 512\n",
      "Embedding: [-0.02833266742527485, -0.0558621808886528, -0.012941486202180386, ...]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word = \"Elephant\"\n",
    "sentence = \"I am a sentence for which I would like to get its embedding.\"\n",
    "paragraph = (\n",
    "    \"Universal Sentence Encoder embeddings also support short paragraphs. \"\n",
    "    \"There is no hard limit on how long the paragraph is. Roughly, the longer \"\n",
    "    \"the more 'diluted' the embedding will be.\")\n",
    "messages = [word, sentence, paragraph]\n",
    "\n",
    "# Reduce logging output.\n",
    "logging.set_verbosity(logging.ERROR)\n",
    "\n",
    "message_embeddings = embed(messages)\n",
    "\n",
    "\n",
    "for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n",
    "  print(\"Message: {}\".format(messages[i]))\n",
    "  print(\"Embedding size: {}\".format(len(message_embedding)))\n",
    "  message_embedding_snippet = \", \".join(\n",
    "      (str(x) for x in message_embedding[:3]))\n",
    "  print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_similarity(labels, features, rotation):\n",
    "  corr = np.inner(features, features)\n",
    "  sns.set(font_scale=1.2)\n",
    "  g = sns.heatmap(\n",
    "      corr,\n",
    "      xticklabels=labels,\n",
    "      yticklabels=labels,\n",
    "      vmin=0,\n",
    "      vmax=1,\n",
    "      cmap=\"YlOrRd\")\n",
    "  g.set_xticklabels(labels, rotation=rotation)\n",
    "  g.set_title(\"Semantic Textual Similarity\")\n",
    "\n",
    "def run_and_plot(messages_):\n",
    "  message_embeddings_ = embed(messages_)\n",
    "  plot_similarity(messages_, message_embeddings_, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting plagiarism detection...\n",
      "\n",
      "\n",
      "(1, 95, 512) (1, 15, 512)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. check_pairwise_arrays expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\diego\\OneDrive\\Documentos\\plagiarismDetector\\plagiarismDetector\\decision2.ipynb Cell 4\u001b[0m in \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/diego/OneDrive/Documentos/plagiarismDetector/plagiarismDetector/decision2.ipynb#X14sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m embeddings_ori \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(embeddings_ori)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/diego/OneDrive/Documentos/plagiarismDetector/plagiarismDetector/decision2.ipynb#X14sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mprint\u001b[39m(embeddings_ori\u001b[39m.\u001b[39mshape, embeddings_sus\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/diego/OneDrive/Documentos/plagiarismDetector/plagiarismDetector/decision2.ipynb#X14sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m uni_result \u001b[39m=\u001b[39m pairwise\u001b[39m.\u001b[39;49mcosine_similarity(embeddings_sus)[\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/diego/OneDrive/Documentos/plagiarismDetector/plagiarismDetector/decision2.ipynb#X14sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m tri_result \u001b[39m=\u001b[39m pairwise\u001b[39m.\u001b[39mcosine_similarity(embeddings_sus)[\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/diego/OneDrive/Documentos/plagiarismDetector/plagiarismDetector/decision2.ipynb#X14sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSimilarity between suspicious text \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(i) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m and original text \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(i) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m is: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(uni_result))\n",
      "File \u001b[1;32mc:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:1377\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1342\u001b[0m \u001b[39m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[0;32m   1343\u001b[0m \n\u001b[0;32m   1344\u001b[0m \u001b[39mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[39m    Returns the cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[0;32m   1374\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1375\u001b[0m \u001b[39m# to avoid recursive import\u001b[39;00m\n\u001b[1;32m-> 1377\u001b[0m X, Y \u001b[39m=\u001b[39m check_pairwise_arrays(X, Y)\n\u001b[0;32m   1379\u001b[0m X_normalized \u001b[39m=\u001b[39m normalize(X, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   1380\u001b[0m \u001b[39mif\u001b[39;00m X \u001b[39mis\u001b[39;00m Y:\n",
      "File \u001b[1;32mc:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:146\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[1;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[0;32m    143\u001b[0m     dtype \u001b[39m=\u001b[39m dtype_float\n\u001b[0;32m    145\u001b[0m \u001b[39mif\u001b[39;00m Y \u001b[39mis\u001b[39;00m X \u001b[39mor\u001b[39;00m Y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     X \u001b[39m=\u001b[39m Y \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m    147\u001b[0m         X,\n\u001b[0;32m    148\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[0;32m    149\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    150\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    151\u001b[0m         force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[0;32m    152\u001b[0m         estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m    153\u001b[0m     )\n\u001b[0;32m    154\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m     X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m    156\u001b[0m         X,\n\u001b[0;32m    157\u001b[0m         accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    161\u001b[0m         estimator\u001b[39m=\u001b[39mestimator,\n\u001b[0;32m    162\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:893\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    888\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdtype=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnumeric\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    889\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    890\u001b[0m     )\n\u001b[0;32m    892\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nd \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[1;32m--> 893\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    894\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    895\u001b[0m         \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    896\u001b[0m     )\n\u001b[0;32m    898\u001b[0m \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[0;32m    899\u001b[0m     _assert_all_finite(\n\u001b[0;32m    900\u001b[0m         array,\n\u001b[0;32m    901\u001b[0m         input_name\u001b[39m=\u001b[39minput_name,\n\u001b[0;32m    902\u001b[0m         estimator_name\u001b[39m=\u001b[39mestimator_name,\n\u001b[0;32m    903\u001b[0m         allow_nan\u001b[39m=\u001b[39mforce_all_finite \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mallow-nan\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    904\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with dim 3. check_pairwise_arrays expected <= 2."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sklearn.metrics.pairwise as pairwise\n",
    "\n",
    "file_path_originals = \"original_files\"\n",
    "file_path_suspicious = \"suspicious_files\"\n",
    "\n",
    "# Preprocessing original texts\n",
    "original_texts = [file for file in os.listdir(file_path_originals) if os.path.isfile(os.path.join(file_path_originals, file))]\n",
    "processed_original_texts = []\n",
    "for original_text in original_texts:\n",
    "    processed_original_texts.append(preprocessing(file_path_originals + \"/\" + original_text))\n",
    "\n",
    "print(\"Starting plagiarism detection...\")\n",
    "print(\"\\n\")\n",
    "# Preprocessing suspicious texts\n",
    "suspicious_texts = [file for file in os.listdir(file_path_suspicious) if os.path.isfile(os.path.join(file_path_suspicious, file))]\n",
    "processed_suspicious_texts = []\n",
    "for suspicious_text in suspicious_texts:\n",
    "    processed_suspicious_texts.append(preprocessing(file_path_suspicious + \"/\" + suspicious_text))\n",
    "\n",
    "\n",
    "embeddings_sus = []\n",
    "embeddings_ori = []\n",
    "\n",
    "\n",
    "# for i in range(len(processed_suspicious_texts)):\n",
    "embeddings_sus.append(embed(processed_suspicious_texts))\n",
    "embeddings_ori.append(embed(processed_original_texts))\\\n",
    "\n",
    "# reshape to only have np array\n",
    "embeddings_sus = np.array(embeddings_sus)\n",
    "embeddings_ori = np.array(embeddings_ori)\n",
    "\n",
    "# reshape to only have np array\n",
    "embeddings_sus = np.reshape(embeddings_sus, (1,512))\n",
    "embeddings_ori = np.reshape(embeddings_ori, (1,512))\n",
    "\n",
    "\n",
    "uni_result = pairwise.cosine_similarity(embeddings_sus)[0,1]\n",
    "tri_result = pairwise.cosine_similarity(embeddings_sus)[0,1]\n",
    "print(\"Similarity between suspicious text \" + str(i) + \" and original text \" + str(i) + \" is: \" + str(uni_result))\n",
    "print(\"Similarity between suspicious text \" + str(i) + \" and original text \" + str(i) + \" is: \" + str(tri_result))\n",
    "\n",
    "print(embeddings_ori, embeddings_sus)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
