{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "from preprocess import preprocessing\n",
    "from preparation import preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printConfussionMatrix(y_test, y_pred):\n",
    "    sns.heatmap((confusion_matrix(y_test,y_pred)), annot=True, fmt=\"d\",cmap=\"crest\")\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.ylabel('True class')\n",
    "    plt.xlabel('Predicted class')\n",
    "    plt.show()\n",
    "\n",
    "def build_word_frequency_histogram(preprocessed_str):\n",
    "    wordfreq = {}\n",
    "    for word in preprocessed_str.split():\n",
    "        if word not in wordfreq:\n",
    "            wordfreq[word] = 0\n",
    "        wordfreq[word] += 1\n",
    "    return wordfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictPlag = {}\n",
    "#actual_results = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "actual_results = [0,0,0,0,0,0,0,0,0,0,1,1,1,1,1]\n",
    "system_results = []\n",
    "\n",
    "def decision(filePathOriginals, filePathSuspicious):\n",
    "   \n",
    "    # Preprocessing original texts\n",
    "    original_texts = [file for file in os.listdir(filePathOriginals) if os.path.isfile(os.path.join(filePathOriginals, file))]\n",
    "    processed_original_texts = []\n",
    "    print(original_texts)\n",
    "    for original_text in original_texts:\n",
    "        print(original_text)\n",
    "        processed_original_texts.append(preprocessing(filePathOriginals + \"/\" + original_text))\n",
    "    print(original_texts)\n",
    "\n",
    "    print(\"Starting plagiarism detection...\")\n",
    "    print(\"\\n\")\n",
    "    # Preprocessing suspicious texts\n",
    "    suspicious_texts = [file for file in os.listdir(\"suspicious_files\") if os.path.isfile(os.path.join(\"suspicious_files\", file))]\n",
    "    processed_suspicious_texts = []\n",
    "    print(suspicious_texts)\n",
    "    for suspicious_text in suspicious_texts:\n",
    "        processed_suspicious_texts.append(preprocessing(\"suspicious_files/\" + suspicious_text))\n",
    "   \n",
    "    \n",
    "    print(\"len processed suspicious: \", len(processed_original_texts))\n",
    "    #histograms = []\n",
    "    \n",
    "    maxplagiarism = [0 for _ in range(len(processed_suspicious_texts))]\n",
    "    maxhistograms = [{} for _ in range(len(processed_suspicious_texts))]\n",
    "    \n",
    "    # Comparing suspicious text with original texts\n",
    "    for k, processed_suspicious_text in enumerate(processed_suspicious_texts):\n",
    "        print(\"Suspicious text: \", suspicious_text)\n",
    "        plagiarized_check = False\n",
    "        for i, processed_original_text in enumerate(processed_original_texts):\n",
    "            unigram_result, trigram_result = preparation(processed_suspicious_text, processed_original_text)\n",
    "\n",
    "            # If similarity is greater than 0.5 in unigrams and more , then plagiarism is detected\n",
    "            if unigram_result > 0.15 and trigram_result > 0.05:\n",
    "                print(\"Plagiarism detected in file: \", original_texts[i])\n",
    "                print(\"⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️\")\n",
    "                print(\"\\n\")\n",
    "                \n",
    "                if dictPlag.get(suspicious_text) is None:\n",
    "                    dictPlag[suspicious_text] = [[original_texts[i],unigram_result,trigram_result]]\n",
    "                else:\n",
    "                    dictPlag[suspicious_text].append([original_texts[i], unigram_result,trigram_result])\n",
    "                    \n",
    "                \n",
    "                if not plagiarized_check:\n",
    "                    system_results.append(1)\n",
    "                    plagiarized_check = True\n",
    "            \n",
    "                # similarities = find_similarities(suspicious_text, original_texts[i])\n",
    "                # all_similarities.append([original_texts[i], similarities])\n",
    "                # for similarity, position in similarities:\n",
    "                #     print(f\"Similarity: '{similarity}', Position: {position}\")\\\n",
    "\n",
    "                if maxplagiarism[k] <= unigram_result:\n",
    "                    maxplagiarism[k] = unigram_result\n",
    "                    maxhistograms[k] = build_word_frequency_histogram(processed_suspicious_text)\n",
    "                    \n",
    "                 \n",
    "\n",
    "                #histograms.append(build_word_frequency_histogram(processed_suspicious_text))\n",
    "                # categories = list(histogram.keys())[:20]\n",
    "                # frequencies = list(histogram.values())[:20]\n",
    "                \n",
    "                # plt.bar(categories, frequencies)            \n",
    "                # plt.xlabel('Categories')\n",
    "                # plt.ylabel('Frequency')\n",
    "                # plt.title('Category Frequency Histogram')\n",
    "                # plt.xticks(rotation=90)\n",
    "\n",
    "        if not plagiarized_check:\n",
    "            # When there is no plagiarism, append 0 to system results\n",
    "            system_results.append(0)\n",
    "\n",
    "    # print(dictPlag)\n",
    "\n",
    "    maxhistograms = [histogram for histogram in maxhistograms if histogram]\n",
    "\n",
    "    # Create subplots for all bar charts of histograms\n",
    "    fig, axs = plt.subplots(len(maxhistograms)+1, figsize=(10, 70))\n",
    "    fig.tight_layout(pad=10)\n",
    "    for i, histogram in enumerate(maxhistograms):\n",
    "        categories = list(histogram.keys())[:20]\n",
    "        frequencies = list(histogram.values())[:20]\n",
    "        axs[i].bar(categories, frequencies)            \n",
    "        axs[i].set_xlabel('Categories')\n",
    "        axs[i].set_ylabel('Frequency')\n",
    "        axs[i].set_title('Category Frequency Histogram')\n",
    "        axs[i].tick_params(axis='x', rotation=90)\n",
    "\n",
    "\n",
    "    \n",
    "    print(f'SYS {system_results}')\n",
    "    print(f'Actual {actual_results}')\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(actual_results, system_results, pos_label=1)\n",
    "\n",
    "\n",
    "    # RECORDAR ARREGLAR LO DEL ROOOOC\n",
    "\n",
    "    print(\"False Positive Rate: \", fpr)\n",
    "    print(\"True Positive Rate: \", tpr)\n",
    "    print(\"AUC:\", metrics.auc(fpr, tpr))\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.title('ROC curve')\n",
    "    plt.xlabel(\"Error\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.show()\n",
    "    printConfussionMatrix(actual_results, system_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['org-001.txt', 'org-002.txt', 'org-003.txt', 'org-004.txt', 'org-005.txt', 'org-006.txt', 'org-007.txt', 'org-008.txt', 'org-009.txt', 'org-010.txt', 'org-011.txt', 'org-012.txt', 'org-013.txt', 'org-014.txt', 'org-015.txt', 'org-016.txt', 'org-017.txt', 'org-018.txt', 'org-019.txt', 'org-020.txt', 'org-021.txt', 'org-022.txt', 'org-023.txt', 'org-024.txt', 'org-025.txt', 'org-026.txt', 'org-027.txt', 'org-028.txt', 'org-029.txt', 'org-030.txt', 'org-031.txt', 'org-032.txt', 'org-033.txt', 'org-034.txt', 'org-035.txt', 'org-036.txt', 'org-037.txt', 'org-038.txt', 'org-039.txt', 'org-040.txt', 'org-041.txt', 'org-042.txt', 'org-043.txt', 'org-044.txt', 'org-045.txt', 'org-046.txt', 'org-047.txt', 'org-048.txt', 'org-049.txt', 'org-050.txt', 'org-051.txt', 'org-052.txt', 'org-053.txt', 'org-054.txt', 'org-055.txt', 'org-056.txt', 'org-057.txt', 'org-058.txt', 'org-059.txt', 'org-060.txt', 'org-061.txt', 'org-062.txt', 'org-063.txt', 'org-064.txt', 'org-065.txt', 'org-066.txt', 'org-067.txt', 'org-068.txt', 'org-069.txt', 'org-070.txt', 'org-071.txt', 'org-072.txt', 'org-073.txt', 'org-074.txt', 'org-075.txt', 'org-076.txt', 'org-077.txt', 'org-078.txt', 'org-079.txt', 'org-080.txt', 'org-081.txt', 'org-082.txt', 'org-083.txt', 'org-084.txt', 'org-085.txt', 'org-086.txt', 'org-087.txt', 'org-088.txt', 'org-089.txt', 'org-090.txt', 'org-091.txt', 'org-092.txt', 'org-093.txt', 'org-094.txt', 'org-095.txt']\n",
      "org-001.txt\n",
      "org-002.txt\n",
      "org-003.txt\n",
      "org-004.txt\n",
      "org-005.txt\n",
      "org-006.txt\n",
      "org-007.txt\n",
      "org-008.txt\n",
      "org-009.txt\n",
      "org-010.txt\n",
      "org-011.txt\n",
      "org-012.txt\n",
      "org-013.txt\n",
      "org-014.txt\n",
      "org-015.txt\n",
      "org-016.txt\n",
      "org-017.txt\n",
      "org-018.txt\n",
      "org-019.txt\n",
      "org-020.txt\n",
      "org-021.txt\n",
      "org-022.txt\n",
      "org-023.txt\n",
      "org-024.txt\n",
      "org-025.txt\n",
      "org-026.txt\n",
      "org-027.txt\n",
      "org-028.txt\n",
      "org-029.txt\n",
      "org-030.txt\n",
      "org-031.txt\n",
      "org-032.txt\n",
      "org-033.txt\n",
      "org-034.txt\n",
      "org-035.txt\n",
      "org-036.txt\n",
      "org-037.txt\n",
      "org-038.txt\n",
      "org-039.txt\n",
      "org-040.txt\n",
      "org-041.txt\n",
      "org-042.txt\n",
      "org-043.txt\n",
      "org-044.txt\n",
      "org-045.txt\n",
      "org-046.txt\n",
      "org-047.txt\n",
      "org-048.txt\n",
      "org-049.txt\n",
      "org-050.txt\n",
      "org-051.txt\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 116: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\diego\\OneDrive\\Documentos\\plagiarismDetector\\plagiarismDetector\\decision.ipynb Cell 4\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/diego/OneDrive/Documentos/plagiarismDetector/plagiarismDetector/decision.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m decision(\u001b[39m\"\u001b[39;49m\u001b[39moriginal_files\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39msuspicious_files\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\diego\\OneDrive\\Documentos\\plagiarismDetector\\plagiarismDetector\\decision.ipynb Cell 4\u001b[0m in \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/diego/OneDrive/Documentos/plagiarismDetector/plagiarismDetector/decision.ipynb#X23sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m original_text \u001b[39min\u001b[39;00m original_texts:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/diego/OneDrive/Documentos/plagiarismDetector/plagiarismDetector/decision.ipynb#X23sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mprint\u001b[39m(original_text)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/diego/OneDrive/Documentos/plagiarismDetector/plagiarismDetector/decision.ipynb#X23sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     processed_original_texts\u001b[39m.\u001b[39mappend(preprocessing(filePathOriginals \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m original_text))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/diego/OneDrive/Documentos/plagiarismDetector/plagiarismDetector/decision.ipynb#X23sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(original_texts)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/diego/OneDrive/Documentos/plagiarismDetector/plagiarismDetector/decision.ipynb#X23sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mStarting plagiarism detection...\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\diego\\OneDrive\\Documentos\\plagiarismDetector\\plagiarismDetector\\preprocess.py:11\u001b[0m, in \u001b[0;36mpreprocessing\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m      8\u001b[0m     document \u001b[39m=\u001b[39m document\u001b[39m.\u001b[39mread()\n\u001b[0;32m     10\u001b[0m \u001b[39m# Remove prepositions, articles, etc\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparsing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m remove_stopwords\n\u001b[0;32m     13\u001b[0m filtered_doc \u001b[39m=\u001b[39m remove_stopwords(document)\n\u001b[0;32m     15\u001b[0m \u001b[39m# Turn to lowercase\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python310\\lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[39mreturn\u001b[39;00m codecs\u001b[39m.\u001b[39;49mcharmap_decode(\u001b[39minput\u001b[39;49m,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors,decoding_table)[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 116: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "decision(\"original_files\", \"suspicious_files\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
