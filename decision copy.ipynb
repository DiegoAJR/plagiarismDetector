{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used packages\n",
    "from preprocess import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "from preprocess import preprocessing\n",
    "from preparation import preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Plots the confussion matrix from our system results\n",
    "\n",
    "Receives a vector with the system results and another one with the actual results\n",
    "Does not return anything but it displays the confussion matrix\n",
    "'''\n",
    "def print_confussion_matrix(y_test, y_pred):\n",
    "    sns.heatmap((confusion_matrix(y_test,y_pred)), annot=True, fmt=\"d\",cmap=\"crest\")\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.ylabel('True class')\n",
    "    plt.xlabel('Predicted class')\n",
    "    plt.show()\n",
    "\n",
    "'''\n",
    "Builds a dictionary with the word frequency counts of a string\n",
    "\n",
    "Receives a preprocessed text\n",
    "Returns a dictionary with the word frequency counts\n",
    "'''\n",
    "def build_word_frequency_histogram(preprocessed_str):\n",
    "    wordfreq = {}\n",
    "\n",
    "    # If word has already been found, add 1 to its count. If not, add the word to the dictionary\n",
    "    for word in preprocessed_str.split():\n",
    "        if word not in wordfreq:\n",
    "            wordfreq[word] = 0\n",
    "        wordfreq[word] += 1\n",
    "    \n",
    "    # REturn 10 most frequent words\n",
    "    return dict(sorted(wordfreq.items(), key=lambda item: item[1], reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Creates subplots and displays all bar charts of histograms\n",
    "\n",
    "Receives all the system results\n",
    "Does not return anything but it displays all the histograms, ROC Curve and Confussion Matrix\n",
    "'''\n",
    "def create_subplots(maxhistograms, maxhistogramsoriginals, maxfilename, maxfilenameoriginals ,fpr, tpr, actual_results, system_results):\n",
    "    # Plot Histograms\n",
    "    fig, axs = plt.subplots(len(maxhistograms), 2, figsize=(20, 40))\n",
    "    fig.tight_layout(pad=10)\n",
    "    for i, histogram in enumerate(maxhistograms):\n",
    "        categories = list(histogram.keys())[:20]\n",
    "        frequencies = list(histogram.values())[:20]\n",
    "        axs[i][0].bar(categories, frequencies)            \n",
    "        axs[i][0].set_xlabel('Categories')\n",
    "        axs[i][0].set_ylabel('Frequency')\n",
    "        axs[i][0].set_title(f'Word frequency histogram for {maxfilename[i]}')\n",
    "        axs[i][0].tick_params(axis='x', rotation=90)\n",
    "        categories = list(maxhistogramsoriginals[i].keys())[:20]\n",
    "        frequencies = list(maxhistogramsoriginals[i].values())[:20]\n",
    "        axs[i][1].bar(categories, frequencies)\n",
    "        axs[i][1].set_xlabel('Categories')\n",
    "        axs[i][1].set_ylabel('Frequency')\n",
    "        axs[i][1].set_title(f'Word frequency histogram for {maxfilenameoriginals[i]}')\n",
    "        axs[i][1].tick_params(axis='x', rotation=90)\n",
    "\n",
    "    # Plot ROC Curve\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.title('ROC curve')\n",
    "    plt.xlabel(\"Error\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot Confussion Matrix\n",
    "    print_confussion_matrix(actual_results, system_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stores all the plagiarism results for every comparison\n",
    "dictPlag = {}\n",
    "\n",
    "# Actual results of our tests. 1 = Plagiarism, 0 = Original\n",
    "actual_results = [1,1,0,0,1,1,0,0,0,0,0,0,0,0,1]\n",
    "\n",
    "# Results found by our system when doing the plagiarism analysis\n",
    "system_results = []\n",
    "\n",
    "'''\n",
    "Main decision function that handles all the main logic in our system.\n",
    "First, it gathers all the original and suspicious documents for preprocessing.\n",
    "Then, it calls for the calculation of the cosine similarity between a suspicious and an original text.\n",
    "Afterwards, it determines whether there is plagiarism or not and it calculates the word frequency histograms.\n",
    "Finally, it displays all the histograms, ROC Curve and Confussion Matrix.\n",
    "\n",
    "Receives a file path to the original and suspicious files\n",
    "Does not return anything but it concludes by showing all the result graphs.\n",
    "'''\n",
    "def decision(file_path_originals, file_path_suspicious):\n",
    "   \n",
    "    # Preprocessing original texts\n",
    "    original_texts = [file for file in os.listdir(file_path_originals) if os.path.isfile(os.path.join(file_path_originals, file))]\n",
    "    processed_original_texts = []\n",
    "    for original_text in original_texts:\n",
    "        processed_original_texts.append(preprocessing(file_path_originals + \"/\" + original_text))\n",
    "\n",
    "    print(\"Starting plagiarism detection...\")\n",
    "    print(\"\\n\")\n",
    "    # Preprocessing suspicious texts\n",
    "    suspicious_texts = [file for file in os.listdir(file_path_suspicious) if os.path.isfile(os.path.join(file_path_suspicious, file))]\n",
    "    processed_suspicious_texts = []\n",
    "    for suspicious_text in suspicious_texts:\n",
    "        processed_suspicious_texts.append(preprocessing(file_path_suspicious + \"/\" + suspicious_text))\n",
    "       \n",
    "    # Creating histograms for suspicious texts\n",
    "    maxplagiarism = [0 for _ in range(len(processed_suspicious_texts))]\n",
    "    maxhistograms = [{} for _ in range(len(processed_suspicious_texts))]\n",
    "    maxhistogramsoriginal = [{} for _ in range(len(processed_suspicious_texts))]\n",
    "    maxfilename = [\"\" for _ in range(len(processed_suspicious_texts))]\n",
    "    maxfilenameoriginal = [\"\" for _ in range(len(processed_suspicious_texts))]\n",
    "    \n",
    "    # Comparing suspicious text with original texts\n",
    "    for k, processed_suspicious_text in enumerate(processed_suspicious_texts):\n",
    "        print(\"Analyzing Suspicious text: \", suspicious_texts[k])\n",
    "        plagiarized_check = False\n",
    "\n",
    "        for i, processed_original_text in enumerate(processed_original_texts):\n",
    "            unigram_result, trigram_result = preparation(processed_suspicious_text, processed_original_text)\n",
    "\n",
    "            # Check if plagiarism is detected\n",
    "            if unigram_result > 0.15 and trigram_result > 0.15:\n",
    "                print(\"\\tPlagiarism detected in file: \", original_texts[i])\n",
    "                print(f'\\tMaximum percentage of plagiarism: {max(unigram_result,trigram_result)*100:.1f}%')\n",
    "                print(\"\\t⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️\\n\")\n",
    "                \n",
    "                if dictPlag.get(suspicious_text) is None:\n",
    "                    dictPlag[suspicious_text] = [[original_texts[i],unigram_result,trigram_result]]\n",
    "                else:\n",
    "                    dictPlag[suspicious_text].append([original_texts[i], unigram_result,trigram_result])\n",
    "                    \n",
    "                # When plagiarism is detected, append 1 to system results\n",
    "                if not plagiarized_check:\n",
    "                    system_results.append(1)\n",
    "                    plagiarized_check = True\n",
    "                \n",
    "                # Check if plagiarism is the highest detected\n",
    "                if maxplagiarism[k] <= unigram_result:\n",
    "                    maxplagiarism[k] = unigram_result\n",
    "                    maxfilename[k] = suspicious_texts[k]\n",
    "                    maxfilenameoriginal[k] = original_texts[i]\n",
    "                    maxhistograms[k] = build_word_frequency_histogram(processed_suspicious_text)\n",
    "                    maxhistogramsoriginal[k] = build_word_frequency_histogram(processed_original_text)\n",
    "                \n",
    "                    \n",
    "        # When there is no plagiarism, append 0 to system results\n",
    "        if not plagiarized_check:\n",
    "            print(\"\\tNo plagiarism detected\\n\")\n",
    "            system_results.append(0)\n",
    "\n",
    "\n",
    "    # Remove empty histograms\n",
    "    maxhistograms = [histogram for histogram in maxhistograms if histogram]\n",
    "    maxfilename = [filename for filename in maxfilename if filename]\n",
    "    maxfilenameoriginal = [filename for filename in maxfilenameoriginal if filename]\n",
    "    maxhistogramsoriginal = [histogram for histogram in maxhistogramsoriginal if histogram]\n",
    "\n",
    "    # Print results\n",
    "    tn, fp, fn, tp = confusion_matrix(actual_results, system_results).ravel()\n",
    "    print(f'Predicted Results: {system_results}')\n",
    "    print(f'Actual Results: {actual_results}')\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(actual_results, system_results, pos_label=1)\n",
    "    print(f'True Positive: {tp}')\n",
    "    print(f'False Positive: {fp}')\n",
    "    print(f'True Negative: {tn}')\n",
    "    print(f'False Negative: {fn}')\n",
    "    print(\"False Positive Rate: \", fp/(fp+tn))\n",
    "    print(\"True Positive Rate: \", tp/(tp+fn))\n",
    "    print(\"AUC:\", metrics.auc(fpr, tpr))\n",
    "\n",
    "    # Display graphs\n",
    "    create_subplots(maxhistograms, maxhistogramsoriginal ,maxfilename, maxfilenameoriginal ,fpr, tpr, actual_results, system_results)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the main system function to kick-off the plagiarism detector\n",
    "decision(\"original_files\", \"suspicious_files\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
