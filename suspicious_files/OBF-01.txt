The process of training deep neural networks is complicated by a phenomenon known as internal covariate shift, which occurs when the distribution of each layer's inputs changes during training as the parameters of the previous layers change. This slows down training and makes it difficult to train models with saturating nonlinearities. To address this problem, the authors of a paper propose a method called Batch Normalization, which normalizes layer inputs as a part of the model architecture and performs normalization for each training mini-batch. This allows for higher learning rates and less careful parameter initialization, while also acting as a regularizer. The authors applied Batch Normalization to an image classification model, achieving the same accuracy with 14 times fewer training steps and beating the original model by a significant margin. They also improved upon the best published result on ImageNet classification using an ensemble of batch-normalized networks, reaching a top-5 validation error of 4.9% and exceeding the accuracy of human raters.